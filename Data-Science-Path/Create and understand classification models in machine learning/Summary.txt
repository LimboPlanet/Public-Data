We’ve completed our introduction to classification. Let’s recap some key points.

We’ve seen that classification has a lot in common with classic regression. For both we can use supervised learning, a cost function, and use test and train datasets to estimate real-world performance. We focused here on logistic regression, which is almost a hybrid between these two types of model, and showed how thresholding the output gives us a categorical label, like ‘avalanche’/’no-avalanche’.

We discussed how assessing classification models can be slightly more difficult than with regression models, particularly because the costs functions involved are often unintuitive.

We also explored how adding and combining features could result in substantial model improvements. Importantly, we showed how really thinking about what your data mean is the key to achieving the best result.

Remember that while we worked with logistic regression here, most topics we covered apply to many other types of classification models as well, including those that try to predict more than two possible categories.